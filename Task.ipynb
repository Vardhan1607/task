{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "y_QQtGV2x1ts"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNvUZsT5JNkq",
        "outputId": "eb3942ab-9b4f-4d75-edc4-4c686ac76a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tweet-preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkkKA8gxILjV",
        "outputId": "c10c2625-40b1-40cb-ac3c-41e31e640ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import preprocessor as p\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(language='english')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9TNtGSfrx6Vi"
      },
      "outputs": [],
      "source": [
        "def preprocess_tweet(text):\n",
        "    text = p.clean(text)\n",
        "    return text\n",
        "\n",
        "def remove_urls(raw_text):\n",
        "    raw_text = re.sub(r'http\\S+', '', raw_text)\n",
        "    return re.sub(r'www\\S+', '', raw_text)\n",
        "\n",
        "def remove_nonalpha(raw_text):\n",
        "  return re.sub(r'[^a-zA-Z\\s]', '', raw_text)\n",
        "\n",
        "def remove_lines(raw_text):\n",
        "  return re.sub(r'\\n',' ',raw_text)\n",
        "\n",
        "def lemme_stem_stop(raw_text):\n",
        "  words = word_tokenize(raw_text)\n",
        "  final_words=[]\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    if word not in stop_words:\n",
        "      word = snowball_stemmer.stem(word)\n",
        "      final_words.append(word)\n",
        "  return \" \".join(final_words)\n",
        "\n",
        "def pre_processing(raw_text):\n",
        "  return lemme_stem_stop(remove_lines(remove_nonalpha(remove_urls(preprocess_tweet(raw_text)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Q7AauPx2VtKe"
      },
      "outputs": [],
      "source": [
        "class UNet():\n",
        "    def __init__(self,path):\n",
        "        self.path=path\n",
        "\n",
        "    def load_data(self):\n",
        "        data = pd.read_csv(self.path)\n",
        "        data['text'] = data['text'].apply(lambda x: pre_processing(x))\n",
        "        self.df = data\n",
        "        self.length = self.df['text'].apply(lambda x: len(x.split(' ')))\n",
        "        reviews = self.df[\"text\"]\n",
        "        labels = self.df[\"airline_sentiment\"]\n",
        "        encoder = LabelEncoder()\n",
        "        self.encoded_labels = encoder.fit_transform(labels)\n",
        "        self.train_sentences, self.test_sentences, self.train_labels, self.test_labels = train_test_split(reviews, self.encoded_labels, test_size=0.2, random_state=123)\n",
        "\n",
        "\n",
        "    def _preprocess(self):\n",
        "        self.vocab_size = 3000\n",
        "        self.oov_tok = '<OOK>'\n",
        "        self.embedding_dim = 100\n",
        "        self.max_length = 150\n",
        "        self.padding_type='post'\n",
        "        self.trunc_type='post'\n",
        "        self.tokenizer = Tokenizer(num_words = self.vocab_size, oov_token=self.oov_tok)\n",
        "        self.tokenizer.fit_on_texts(self.train_sentences)\n",
        "        word_index = self.tokenizer.word_index\n",
        "        train_sequences = self.tokenizer.texts_to_sequences(self.train_sentences)\n",
        "        self.train_padded = pad_sequences(train_sequences, padding='post', maxlen =self.max_length)\n",
        "        test_sequences = self.tokenizer.texts_to_sequences(self.test_sentences)\n",
        "        self.test_padded = pad_sequences(test_sequences, padding='post', maxlen=self.max_length)\n",
        "\n",
        "    def build(self):\n",
        "      self._preprocess()\n",
        "      self.model =keras.Sequential([\n",
        "          keras.layers.Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_length),\n",
        "          keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "          keras.layers.Dense(24, activation='relu'),\n",
        "          keras.layers.Dense(1, activation='sigmoid')])\n",
        "      self.model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    def train(self):\n",
        "        self.history = self.model.fit(self.train_padded, self.train_labels, \n",
        "                    epochs=5, verbose=1,\n",
        "                    validation_split=0.1)\n",
        "        \n",
        "    def evaluate(self):\n",
        "      prediction = self.model.predict(self.test_padded)\n",
        "      predict=[]\n",
        "      for i in prediction:\n",
        "        if i>=0.5:\n",
        "          predict.append(1)\n",
        "        else:\n",
        "          predict.append(0)\n",
        "      return \"Accuracy of the built model is \"+str(accuracy_score(predict,self.test_labels)*100)+\" %\"\n",
        "\n",
        "    def prediction(self,sentences):\n",
        "          processed_sentences=[]\n",
        "          for sent in sentences:\n",
        "            processed_sentences.append(pre_processing(sent))\n",
        "            sequences = self.tokenizer.texts_to_sequences(processed_sentences)\n",
        "            padded =  pad_sequences(sequences, padding='post', maxlen=self.max_length)\n",
        "\n",
        "          predict = self.model.predict(padded)\n",
        "          test_sent_prob=[]\n",
        "          for i in predict:\n",
        "            if i>=0.5:\n",
        "              test_sent_prob.append(1)\n",
        "            else:\n",
        "              test_sent_prob.append(0)\n",
        "          return test_sent_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "a5zzeFfEaRUc"
      },
      "outputs": [],
      "source": [
        "def run(path):  \n",
        "  model = UNet(path)\n",
        "  model.load_data()\n",
        "  model.build()\n",
        "  model.train()\n",
        "  print(model.evaluate())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC1ONtaEfpJ6",
        "outputId": "a5eac057-38bc-4c8e-96ed-7879ff4c004b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "260/260 [==============================] - 63s 214ms/step - loss: 0.3261 - accuracy: 0.8647 - val_loss: 0.2465 - val_accuracy: 0.9069\n",
            "Epoch 2/5\n",
            "260/260 [==============================] - 56s 216ms/step - loss: 0.1663 - accuracy: 0.9342 - val_loss: 0.2591 - val_accuracy: 0.9069\n",
            "Epoch 3/5\n",
            "260/260 [==============================] - 52s 199ms/step - loss: 0.1268 - accuracy: 0.9526 - val_loss: 0.2992 - val_accuracy: 0.8972\n",
            "Epoch 4/5\n",
            "260/260 [==============================] - 52s 200ms/step - loss: 0.1023 - accuracy: 0.9632 - val_loss: 0.3382 - val_accuracy: 0.8907\n",
            "Epoch 5/5\n",
            "260/260 [==============================] - 52s 200ms/step - loss: 0.0866 - accuracy: 0.9683 - val_loss: 0.3597 - val_accuracy: 0.8972\n",
            "Accuracy of the built model is 90.34213945430922 %\n"
          ]
        }
      ],
      "source": [
        "path = \"/content/airline_sentiment_analysis.csv\"\n",
        "model = run(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxmuIej-hbFC",
        "outputId": "c76ea2b3-3c51-4360-c4f9-6beb8621297e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "sentences = [\"@VirginAmerica yes, nearly every time I fly VX this ear worm wont go away :)\", \n",
        "            \"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA\", \n",
        "            \"@VirginAmerica it was amazing, and arrived an hour early. Youre too good to me.\"]\n",
        "\n",
        "model.prediction(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWGrEDbsi1yT",
        "outputId": "e4d36a66-8eea-4be8-a860-7a05979fe0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (0.82.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (1.5.5)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.1.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.7/dist-packages (0.18.3)\n",
            "Requirement already satisfied: starlette==0.19.1 in /usr/local/lib/python3.7/dist-packages (from fastapi) (0.19.1)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from fastapi) (1.9.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (3.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (4.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (2.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (1.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn) (7.1.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn) (0.13.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "L9Xrbtzgk5sr"
      },
      "outputs": [],
      "source": [
        "def sentiment_analysis(sentence):\n",
        "  prediction = model.prediction([sentence])\n",
        "  if prediction[0]==0:\n",
        "    return \"Negative\"\n",
        "  else:\n",
        "    return \"Positive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "gdtexZu9radD"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from fastapi import FastAPI\n",
        "import json\n",
        "\n",
        "class senti(BaseModel):\n",
        "  sentence: str\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get('/')\n",
        "def index():\n",
        "    return {'message': 'This is the service for performing sentiment analysis'}\n",
        "\n",
        "@app.post('/predict')\n",
        "def predict_sentiment(data:senti):\n",
        "    prediction = sentiment_analysis(data.sentence)\n",
        "    return {\n",
        "        'prediction': prediction\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn9YzCSDrwBT",
        "outputId": "ef631ddb-2554-4478-fbce-7a5ca5452349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://1951-34-133-40-188.ngrok.io\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [54]\n",
            "INFO:uvicorn.error:Started server process [54]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:uvicorn.error:Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:uvicorn.error:Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "INFO:uvicorn.error:Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     54.86.50.139:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     54.86.50.139:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:uvicorn.error:Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:uvicorn.error:Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:uvicorn.error:Application shutdown complete.\n",
            "INFO:     Finished server process [54]\n",
            "INFO:uvicorn.error:Finished server process [54]\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}